{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpj/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "\n",
    "divorce_model=Doc2Vec.load(\"divorce/divorce.model\") #divorce的Doc2Vec模型\n",
    "loan_model=Doc2Vec.load(\"loan/loan.model\")#loan的Doc2Vec模型\n",
    "labor_model=Doc2Vec.load(\"labor/labor.model\")#labor的Doc2Vec模型\n",
    "\n",
    "data_input = {1: \"divorce/data.txt\", 2: \"labor/data.txt\", 3: \"loan/data.txt\"}\n",
    "models={1:divorce_model,2:loan_model,3:labor_model}\n",
    "data_type = {1: \"divorce\", 2: \"labor\", 3: \"loan\"}\n",
    "label_size = 20\n",
    "test_ratio = 0.3\n",
    "punction = \"！？。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "\n",
    "def line_processing(line):  # 提取每行数据的文本内容\n",
    "    line = line.strip().split('\\t')\n",
    "    sentence = line[1]\n",
    "    sentence = re.sub(r'[{}]'.format(punction),' ',sentence).split(' ')\n",
    "    sent=[]\n",
    "    for sub_sentence in sentence:\n",
    "        if sub_sentence!='':\n",
    "            sent.extend(list(jieba.cut(sub_sentence)))\n",
    "    return line[0], sent, line[2]\n",
    "\n",
    "def constructDataSet(data_path,model_tag): #构建X，Y的数据集\n",
    "    data_file = open(data_path,'r',encoding='utf-8')\n",
    "    lines = data_file.read().splitlines()\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    d2v = models[model_tag]\n",
    "    for line in lines:\n",
    "        _,x,y = line_processing(line)\n",
    "        x=d2v.infer_vector(x)\n",
    "        X.append(x)\n",
    "        y = list(map(int,y.split()))\n",
    "        Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "    data_file.close()\n",
    "    return X,Y\n",
    "\n",
    "def splitDataSet(X,Y,modify=False,ratio=.2): #构建训练集和测试集\n",
    "    if modify: #调整正负样本比例\n",
    "        X_true=[]\n",
    "        X_false=[]\n",
    "        Y_true=[]\n",
    "        Y_false=[]\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i]==1:\n",
    "                X_true.append(X[i])\n",
    "                Y_true.append(Y[i])\n",
    "            else:\n",
    "                X_false.append(X[i])\n",
    "                Y_false.append(Y[i])\n",
    "        true_num = len(X_true)\n",
    "        false_num = true_num*ration\n",
    "        for i in range(0,len(X_false),int(len(X_false)/false_num)):\n",
    "            X_true.append(X_false[i])\n",
    "            Y_true.append(Y_false[i])\n",
    "        X_train,X_test,Y_train,Y_test =  train_test_split(X_true,Y_true,test_size=test_ratio,shuffle=True)\n",
    "    else:\n",
    "        X_train,X_test,Y_train,Y_test =  train_test_split(X,Y,test_size=test_ratio,shuffle=True)\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beginTrain():\n",
    "    for i in range(1,4):\n",
    "        print(data_input[i].split(\"/\")[0])\n",
    "        X,Y = constructDataSet(data_input[i],i)\n",
    "        X_train,X_test,Y_train,Y_test = splitDataSet(X,Y)\n",
    "        base_classifier = SVC(gamma='auto')\n",
    "        chains = [ClassifierChain(base_classifier,order='random',random_state=j) for j in range(10)]\n",
    "        for chain in chains:\n",
    "            chain.fit(X_train,Y_train)\n",
    "        chain_predit = np.array([chain.predict(X_test) for chain in chains])\n",
    "        predict_label = chain_predict.mean(axis=0)\n",
    "        res = []\n",
    "        for j in predict_label:\n",
    "            if j>.5:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(0)\n",
    "        f1score+=f1_score(Y_test,res,average='macro')\n",
    "        print(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.904 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "X,Y = constructDataSet(data_input[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = splitDataSet(X,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
